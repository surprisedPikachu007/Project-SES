{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader('/home/aravind-pt7506/college/project/ollama-examples/gcse/scientific papers/papers/2402.00905.pdf')\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aravind-pt7506/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      ".gitattributes: 100%|██████████| 1.18k/1.18k [00:00<00:00, 8.93MB/s]\n",
      "1_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 875kB/s]\n",
      "README.md: 100%|██████████| 10.6k/10.6k [00:00<00:00, 16.3MB/s]\n",
      "config.json: 100%|██████████| 571/571 [00:00<00:00, 4.21MB/s]\n",
      "config_sentence_transformers.json: 100%|██████████| 116/116 [00:00<00:00, 713kB/s]\n",
      "data_config.json: 100%|██████████| 39.3k/39.3k [00:00<00:00, 6.35MB/s]\n",
      "pytorch_model.bin: 100%|██████████| 438M/438M [02:15<00:00, 3.22MB/s] \n",
      "sentence_bert_config.json: 100%|██████████| 53.0/53.0 [00:00<00:00, 450kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 1.77MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.08MB/s]\n",
      "tokenizer_config.json: 100%|██████████| 363/363 [00:00<00:00, 2.13MB/s]\n",
      "train_script.py: 100%|██████████| 13.1k/13.1k [00:00<00:00, 41.9MB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 4.12MB/s]\n",
      "modules.json: 100%|██████████| 349/349 [00:00<00:00, 933kB/s]\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma.from_documents(texts, embeddings)\n",
    "retriever = db.as_retriever(search_kwargs={'k': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"mistral:7b-instruct-v0.2-q4_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = ConversationalRetrievalChain.from_llm(llm, retriever,return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'Summarize the whole document with key points and actual technical words. include any formulas and tables and images deemed necessary', 'chat_history': [], 'answer': ' Title: Improving Code Quality using GPT-3.5 for Zero-Shot and Few-Shot Learning\\n\\nKey Points:\\n1. The study explores the application of GPT-3.5, a large language model from OpenAI, to improve software code quality through zero-shot and few-shot learning.\\n2. The authors provide prompt templates for both zero-shot and few-shot learning, which consist of persona, instruction, input (submitted code and reviewer comment), and output (improved code).\\n3. For zero-shot learning, the model is given a single input prompt consisting of the submitted code with a reviewer comment if available, and its task is to generate improved code based on that.\\n4. For few-shot learning, the model is provided with three example pairs of input (submitted code with a reviewer comment) and output (improved code), and its task is to learn from these examples and generate improved code for new inputs based on that knowledge.\\n5. The authors use BM25 from the gensim package for selecting relevant examples to help the model adapt to new tasks during few-shot learning.\\n\\nTechnical Details:\\n1. Zero-shot Learning Prompt Template: Persona (expert software developer), Instruction (\"Your task is to improve given code\"), Input (submitted code with a reviewer comment), Output (improved code).\\n2. Few-Shot Learning Prompt Template: Persona (expert software developer), Instruction (\"Your task is to improve given code\"), Input (submitted code with a reviewer comment and three selected examples), Output (improved code).\\n3. Example selection using BM25 from gensim package.\\n\\nFormula/Table/Image: N/A\\n\\nThe authors propose using GPT-3.5, a large language model from OpenAI, to improve software code quality by applying zero-shot and few-shot learning techniques. The study provides prompt templates for both approaches that include persona, instruction, input (submitted code with reviewer comment if available), and output (improved code). For zero-shot learning, the model is given a single input prompt consisting of submitted code with a reviewer comment if available, while for few-shot learning, it\\'s provided with three example pairs to help adapt to new tasks. The authors use BM25 from gensim package for selecting relevant examples during few-shot learning.', 'source_documents': [Document(page_content='Table 5: (RQ3) The evaluation results of GPT-3.5 Zero-shot , GPT-3.5 Few-shot , and GPT-3.5 Fine-tuned (persona is included in input prompts).\\nApproachCodeReviewer data Tufano data(with comment) Tufano data(without comment) Android Google Ovirt\\nEM CodeBLEU EM CodeBLEU EM CodeBLEU EM CodeBLEU EM CodeBLEU EM CodeBLEU\\nGPT-3.5 Zero-shot 17.07% 43.11% 12.49% 77.32% 2.29% 73.21% 0.57% 55.88% 0.00% 50.65% 0.22% 45.73%\\nGPT-3.5 Few-shot 26.28% 47.43% 20.03% 81.61% 9.18% 78.98% 1.62% 74.65% 2.45% 81.07% 1.67% 73.29%\\nGPT-3.5 Fine-tuned 37.70% 49.20% 21.98% 83.04% 6.04% 79.76% 2.29% 74.74% 6.14% 81.02% 2.64% 74.95%\\nTable 6: (RQ3) The evaluation results of GPT-3.5 Zero-shot , GPT-3.5 Few-shot , and GPT-3.5 Fine-tuned (persona is not included in input prompts).\\nApproachCodeReviewer data Tufano data(with comment) Tufano data(without comment) Android Google Ovirt\\nEM CodeBLEU EM CodeBLEU EM CodeBLEU EM CodeBLEU EM CodeBLEU EM CodeBLEU\\nGPT-3.5 Zero-shot 17.72% 44.17% 13.52% 78.36% 2.62% 74.92% 0.49% 61.85% 0.16% 61.04% 0.48% 56.55%\\nGPT-3.5 Few-shot 26.55% 47.50% 19.79% 81.47% 8.96% 79.21% 2.34% 75.33% 2.89% 81.40% 1.64% 73.83%\\nGPT-3.5 Fine-tuned 37.93% 49.00% 22.16% 82.99% 6.02% 79.81% 2.34% 74.15% 6.71% 81.08% 3.05% 74.67%\\nCodeReviewerdata Tufanodata (with comment) Tufanodata (without comment) D−ACTdata\\nFixing bug Refactoring Other Fixing bug Refactoring Other Fixing bug Refactoring Other Fixing bug Refactoring Other01020304050GPT−3.5 Zero−shot GPT−3.5 Few−shot GPT−3.5 Fine−tuned\\n(a) The results of GPT-3.5 (persona is included in input prompts)\\nCodeReviewerdata Tufanodata (with comment) Tufanodata (without comment) D−ACTdata\\nFixing bug Refactoring Other Fixing bug Refactoring Other Fixing bug Refactoring Other Fixing bug Refactoring Other02040GPT−3.5 Zero−shot GPT−3.5 Few−shot GPT−3.5 Fine−tuned\\n(b) The results of GPT-3.5 (persona is not included in input prompts)\\nFigure 6: (RQ3) The EM achieved by GPT-3.5 Zero-shot , GPT-3.5 Few-shot and GPT-3.5 Fine-tuned categorized by types of code change.\\nexisting code review automation approaches?\\nApproach. To address this RQ, we compare GPT-3.5 Zero-shot ,\\nGPT-3.5 Few-shot and GPT-3.5 Fine-tuned with the existing code re-\\nview automation approaches (i.e., TufanoT5 [22], D-ACT [24]\\nand CodeReviewer [23]). To do so, we select the results of GPT-\\n3.5Zero-shot , GPT-3.5 Few-shot and GPT-3.5 Fine-tuned that is gener-\\nated from input prompts without persona. We select such results\\nsince they achieve higher EM and CodeBLEU than the ones\\nthat are generated from input prompts with persona (as shown\\nin RQ2). Then, we obtain the results of the existing code review\\nautomation approaches. In particular, we obtain the results of\\nTufanoT5 and D-ACT from the replication packages. However,\\nsince Li et al. [23] do not make their results publicly available,\\nwe obtain the results from Guo et al. [54] instead. Similar to\\nRQ1-RQ3, we measure EM and CodeBLEU of the results ob-\\ntained from GPT-3.5 and the existing code review automation\\napproaches.\\nResult. The existing code review automation approaches\\nachieve at least 5.47% higher EM than GPT-3.5 that zero-shot learning is performed while fine-tuned GPT-3.5 achieves\\nat least 11.48% higher EM than the existing code review\\nautomation approaches. Table 7 shows the results of GPT-\\n3.5Zero-shot , GPT-3.5 Few-shot , GPT-3.5 Fine-tuned and the existing co-\\nde review automation approaches. The table shows that the\\nexisting code review automation approaches achieve at least\\n5.47% and 1.43% higher EM and CodeBLEU than GPT-3.5 Zero-shot ,\\nrespectively, indicating that the existing code review automa-\\ntion approaches can generate more correct improved code and\\ngenerate more similar improved code to actual improved code\\nthan performing zero-shot learning with GPT-3.5.\\nWe observe from the table that in terms of EM, CodeRe-\\nviewer and D-ACT achieve at least 9.15% higher than GPT-\\n3.5Few-shot . In contrast, GPT-3.5 Few-shot achieves at least 38.78%\\nhigher EM than TufanoT5. In terms of CodeBLEU, CodeRe-\\nviewer and D-ACT achieve at least 0.55% higher than GPT-\\n3.5Few-shot . On the contrary, GPT-3.5 Few-shot achieves at least\\n2.5% higher CodeBLEU than TufanoT5. The results indicate\\nthat performing few-shot learning with GPT-3.5 does not al-\\n8', metadata={'page': 7, 'source': '/home/aravind-pt7506/college/project/ollama-examples/gcse/scientific papers/papers/2402.00905.pdf'}), Document(page_content='Table 1: A statistic of the studied datasets (the dataset of Android, Google and Ovirt are from D-ACT data[24]).\\nDataset # Train # Validation # Test # Language Granularity Has Comment\\nCodeReviewer data[23] 150,405 13,102 13,104 9 DiffHunk Yes\\nTufano data[22] 134,238 16,779 16,779 1 Function Yes/No\\nAndroid [24] 14,690 1,836 1,835 1 Function No\\nGoogle [24] 9,899 1,237 1,235 1 Function No\\nOvirt [24] 21,509 2,686 2,688 1 Function No\\n(Persona) You are an expert software developer in <lang>. You always want to improve your code to have higher quality.(Instruction) Your task is to improve the given submitted code based on the given reviewer comment. Please only generate the improved code without your explanation.(Input) <input code>(Optional Input) //<input comment>\\n(a) A prompt template for zero-shot learning (when a reviewer comment is available).\\n(Persona) You are an expert software developer in <lang>. You always want to improve your code to have higher quality.(Instruction) Your task is to improve the given submitted code. Please only generate the improved code without your explanation.(Input) <input code>\\n(b) A prompt template for zero-shot learning (when a reviewer comment is not available).\\n(Persona) You are an expert software developer in <lang>. You always want to improve your code to have higher quality. You have to generate an output that follows the given examples.(Instruction and examples) You are given 3 examples. Each example begins with \"##Example\" and ends with \"---\". Each example contains the submitted code, the developer comment, and the improved code. The submitted code and improved code is written in <lang>. Your task is to improve your submitted code based on the comment that another developer gave you.## ExampleSubmitted code: <code>Developer comment: <comment>Improved code: <code>--<other examples>--(Input) Submitted code: <input code>(Input) Developer comment: <input comment>\\n(c) A prompt template for few-shot learning (when a reviewer comment is available).\\n(Persona) You are an expert software developer in <lang>. You always want to improve your code to have higher quality. You have to generate an output that follows the given examples.(Instruction and examples) You are given 3 examples. Each example begins with \"##Example\" and ends with \"---\". Each example contains the submitted code and the improved code. The submitted code and improved code is written in <lang>. Your task is to improve your submitted code.## ExampleSubmitted code: <code>Improvedcode: <code>--<other examples>--(Input) Submitted code: <input code>\\n(d) A prompt template for few-shot learning (when a reviewer comment is not available).\\nFigure 2: Prompt templates for performing zero-shot learning and few-shot\\nlearning with GPT-3.5 ( lang refers to a programming language).\\nlowing the guidelines from OpenAI34to ensure that the struc-\\n3https: //help.openai.com /en/articles /6654000-best-practices-for-prompt-\\nengineering-with-openai-api\\n4https: //platform.openai.com /docs/guides /prompt-engineering /strategy-\\nwrite-clear-instructionsture of the prompt is suitable for GPT-3.5. In particular, the\\nprompt template for zero-shot learning (as depicted in Figure 2a\\nand Figure 2b) consists of the following components: perso-\\nna [26], instruction and input (i.e., submitted code). The input\\nalso includes a reviewer comment if available. On the other\\nhand, the prompt template for few-shot learning (as depicted in\\nFigure 2c and Figure 2d) consists of components similar to the\\nones in the prompt template for zero-shot learning, and a set of\\ninput (i.e., submitted code and a reviewer comment) and output\\n(i.e., improved code) example pairs. We include persona in both\\nprompt templates to instruct GPT-3.5 to act as a software devel-\\noper. We do so to ensure that the improved code generated by\\nGPT-3.5 looks like the source code written by a software devel-\\noper. For the prompt template for few-shot learning, we provide\\na set of example pairs to help GPT-3.5 adapt to the new task that\\nit has never learned before.\\n3.3. Zero-Shot Learning\\nTo perform zero-shot learning with GPT-3.5, we construct\\ninput prompts from the prompt template in Figure 2a and the\\ncode submitted for review with a reviewer comment in the test-\\ning set. However, in case a reviewer comment is not available,\\nwe construct input prompts from the prompt template in Fig-\\nure 2b and the code submitted for review instead. Then, we\\nask GPT-3.5 to generate the improved code from a given input\\nprompt.\\n3.4. Few-Shot Learning\\nTo perform few-shot learning with GPT-3.5, we first obtain\\nthree example pairs of input and output from the training set\\nby using BM25 [25] provided by the gensim5package. We do\\nso since prior work [52, 53] shows that BM25 [25] can outper-\\nform other sample selection approaches. Then, we construct\\ninput prompts from the prompt template in Figure 2c, the code\\nsubmitted for review and reviewer comment in the testing set,\\nand its corresponding selected examples. In case a reviewer\\ncomment is not available, we construct input prompts from the\\nprompt template in Figure 2d, the code submitted for review in\\nthe testing set, and its corresponding selected examples instead.\\nAfter that, we ask GPT-3.5 to generate the improved code from\\na given input prompt.\\n5https: //github.com /piskvorky /gensim\\n4', metadata={'page': 3, 'source': '/home/aravind-pt7506/college/project/ollama-examples/gcse/scientific papers/papers/2402.00905.pdf'})]}\n"
     ]
    }
   ],
   "source": [
    "query = \"Summarize the whole document with key points and actual technical words. include any formulas and tables and images deemed necessary\"\n",
    "chat_history = []\n",
    "res = qa_chain({'question': query, 'chat_history': chat_history})\n",
    "print(res)\n",
    "\n",
    "# import sys\n",
    "# chat_history = []\n",
    "# while True:\n",
    "#     query = input('Prompt: ')\n",
    "#     #To exit: use 'exit', 'quit', 'q', or Ctrl-D.\",\n",
    "#     if query.lower() in [\"exit\", \"quit\", \"q\"]:\n",
    "#         print('Exiting')\n",
    "#         sys.exit()\n",
    "#     result = qa_chain({'question': query, 'chat_history': chat_history})\n",
    "#     print('Answer: ' + result['answer'] + '\\n')\n",
    "#     chat_history.append((query, result['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Summarize the whole document with key points and actual technical words. include any formulas and tables and images deemed necessary',\n",
       " 'chat_history': [],\n",
       " 'answer': ' Title: Improving Code Quality using GPT-3.5 for Zero-Shot and Few-Shot Learning\\n\\nKey Points:\\n1. The study explores the application of GPT-3.5, a large language model from OpenAI, to improve software code quality through zero-shot and few-shot learning.\\n2. The authors provide prompt templates for both zero-shot and few-shot learning, which consist of persona, instruction, input (submitted code and reviewer comment), and output (improved code).\\n3. For zero-shot learning, the model is given a single input prompt consisting of the submitted code with a reviewer comment if available, and its task is to generate improved code based on that.\\n4. For few-shot learning, the model is provided with three example pairs of input (submitted code with a reviewer comment) and output (improved code), and its task is to learn from these examples and generate improved code for new inputs based on that knowledge.\\n5. The authors use BM25 from the gensim package for selecting relevant examples to help the model adapt to new tasks during few-shot learning.\\n\\nTechnical Details:\\n1. Zero-shot Learning Prompt Template: Persona (expert software developer), Instruction (\"Your task is to improve given code\"), Input (submitted code with a reviewer comment), Output (improved code).\\n2. Few-Shot Learning Prompt Template: Persona (expert software developer), Instruction (\"Your task is to improve given code\"), Input (submitted code with a reviewer comment and three selected examples), Output (improved code).\\n3. Example selection using BM25 from gensim package.\\n\\nFormula/Table/Image: N/A\\n\\nThe authors propose using GPT-3.5, a large language model from OpenAI, to improve software code quality by applying zero-shot and few-shot learning techniques. The study provides prompt templates for both approaches that include persona, instruction, input (submitted code with reviewer comment if available), and output (improved code). For zero-shot learning, the model is given a single input prompt consisting of submitted code with a reviewer comment if available, while for few-shot learning, it\\'s provided with three example pairs to help adapt to new tasks. The authors use BM25 from gensim package for selecting relevant examples during few-shot learning.',\n",
       " 'source_documents': [Document(page_content='Table 5: (RQ3) The evaluation results of GPT-3.5 Zero-shot , GPT-3.5 Few-shot , and GPT-3.5 Fine-tuned (persona is included in input prompts).\\nApproachCodeReviewer data Tufano data(with comment) Tufano data(without comment) Android Google Ovirt\\nEM CodeBLEU EM CodeBLEU EM CodeBLEU EM CodeBLEU EM CodeBLEU EM CodeBLEU\\nGPT-3.5 Zero-shot 17.07% 43.11% 12.49% 77.32% 2.29% 73.21% 0.57% 55.88% 0.00% 50.65% 0.22% 45.73%\\nGPT-3.5 Few-shot 26.28% 47.43% 20.03% 81.61% 9.18% 78.98% 1.62% 74.65% 2.45% 81.07% 1.67% 73.29%\\nGPT-3.5 Fine-tuned 37.70% 49.20% 21.98% 83.04% 6.04% 79.76% 2.29% 74.74% 6.14% 81.02% 2.64% 74.95%\\nTable 6: (RQ3) The evaluation results of GPT-3.5 Zero-shot , GPT-3.5 Few-shot , and GPT-3.5 Fine-tuned (persona is not included in input prompts).\\nApproachCodeReviewer data Tufano data(with comment) Tufano data(without comment) Android Google Ovirt\\nEM CodeBLEU EM CodeBLEU EM CodeBLEU EM CodeBLEU EM CodeBLEU EM CodeBLEU\\nGPT-3.5 Zero-shot 17.72% 44.17% 13.52% 78.36% 2.62% 74.92% 0.49% 61.85% 0.16% 61.04% 0.48% 56.55%\\nGPT-3.5 Few-shot 26.55% 47.50% 19.79% 81.47% 8.96% 79.21% 2.34% 75.33% 2.89% 81.40% 1.64% 73.83%\\nGPT-3.5 Fine-tuned 37.93% 49.00% 22.16% 82.99% 6.02% 79.81% 2.34% 74.15% 6.71% 81.08% 3.05% 74.67%\\nCodeReviewerdata Tufanodata (with comment) Tufanodata (without comment) D−ACTdata\\nFixing bug Refactoring Other Fixing bug Refactoring Other Fixing bug Refactoring Other Fixing bug Refactoring Other01020304050GPT−3.5 Zero−shot GPT−3.5 Few−shot GPT−3.5 Fine−tuned\\n(a) The results of GPT-3.5 (persona is included in input prompts)\\nCodeReviewerdata Tufanodata (with comment) Tufanodata (without comment) D−ACTdata\\nFixing bug Refactoring Other Fixing bug Refactoring Other Fixing bug Refactoring Other Fixing bug Refactoring Other02040GPT−3.5 Zero−shot GPT−3.5 Few−shot GPT−3.5 Fine−tuned\\n(b) The results of GPT-3.5 (persona is not included in input prompts)\\nFigure 6: (RQ3) The EM achieved by GPT-3.5 Zero-shot , GPT-3.5 Few-shot and GPT-3.5 Fine-tuned categorized by types of code change.\\nexisting code review automation approaches?\\nApproach. To address this RQ, we compare GPT-3.5 Zero-shot ,\\nGPT-3.5 Few-shot and GPT-3.5 Fine-tuned with the existing code re-\\nview automation approaches (i.e., TufanoT5 [22], D-ACT [24]\\nand CodeReviewer [23]). To do so, we select the results of GPT-\\n3.5Zero-shot , GPT-3.5 Few-shot and GPT-3.5 Fine-tuned that is gener-\\nated from input prompts without persona. We select such results\\nsince they achieve higher EM and CodeBLEU than the ones\\nthat are generated from input prompts with persona (as shown\\nin RQ2). Then, we obtain the results of the existing code review\\nautomation approaches. In particular, we obtain the results of\\nTufanoT5 and D-ACT from the replication packages. However,\\nsince Li et al. [23] do not make their results publicly available,\\nwe obtain the results from Guo et al. [54] instead. Similar to\\nRQ1-RQ3, we measure EM and CodeBLEU of the results ob-\\ntained from GPT-3.5 and the existing code review automation\\napproaches.\\nResult. The existing code review automation approaches\\nachieve at least 5.47% higher EM than GPT-3.5 that zero-shot learning is performed while fine-tuned GPT-3.5 achieves\\nat least 11.48% higher EM than the existing code review\\nautomation approaches. Table 7 shows the results of GPT-\\n3.5Zero-shot , GPT-3.5 Few-shot , GPT-3.5 Fine-tuned and the existing co-\\nde review automation approaches. The table shows that the\\nexisting code review automation approaches achieve at least\\n5.47% and 1.43% higher EM and CodeBLEU than GPT-3.5 Zero-shot ,\\nrespectively, indicating that the existing code review automa-\\ntion approaches can generate more correct improved code and\\ngenerate more similar improved code to actual improved code\\nthan performing zero-shot learning with GPT-3.5.\\nWe observe from the table that in terms of EM, CodeRe-\\nviewer and D-ACT achieve at least 9.15% higher than GPT-\\n3.5Few-shot . In contrast, GPT-3.5 Few-shot achieves at least 38.78%\\nhigher EM than TufanoT5. In terms of CodeBLEU, CodeRe-\\nviewer and D-ACT achieve at least 0.55% higher than GPT-\\n3.5Few-shot . On the contrary, GPT-3.5 Few-shot achieves at least\\n2.5% higher CodeBLEU than TufanoT5. The results indicate\\nthat performing few-shot learning with GPT-3.5 does not al-\\n8', metadata={'page': 7, 'source': '/home/aravind-pt7506/college/project/ollama-examples/gcse/scientific papers/papers/2402.00905.pdf'}),\n",
       "  Document(page_content='Table 1: A statistic of the studied datasets (the dataset of Android, Google and Ovirt are from D-ACT data[24]).\\nDataset # Train # Validation # Test # Language Granularity Has Comment\\nCodeReviewer data[23] 150,405 13,102 13,104 9 DiffHunk Yes\\nTufano data[22] 134,238 16,779 16,779 1 Function Yes/No\\nAndroid [24] 14,690 1,836 1,835 1 Function No\\nGoogle [24] 9,899 1,237 1,235 1 Function No\\nOvirt [24] 21,509 2,686 2,688 1 Function No\\n(Persona) You are an expert software developer in <lang>. You always want to improve your code to have higher quality.(Instruction) Your task is to improve the given submitted code based on the given reviewer comment. Please only generate the improved code without your explanation.(Input) <input code>(Optional Input) //<input comment>\\n(a) A prompt template for zero-shot learning (when a reviewer comment is available).\\n(Persona) You are an expert software developer in <lang>. You always want to improve your code to have higher quality.(Instruction) Your task is to improve the given submitted code. Please only generate the improved code without your explanation.(Input) <input code>\\n(b) A prompt template for zero-shot learning (when a reviewer comment is not available).\\n(Persona) You are an expert software developer in <lang>. You always want to improve your code to have higher quality. You have to generate an output that follows the given examples.(Instruction and examples) You are given 3 examples. Each example begins with \"##Example\" and ends with \"---\". Each example contains the submitted code, the developer comment, and the improved code. The submitted code and improved code is written in <lang>. Your task is to improve your submitted code based on the comment that another developer gave you.## ExampleSubmitted code: <code>Developer comment: <comment>Improved code: <code>--<other examples>--(Input) Submitted code: <input code>(Input) Developer comment: <input comment>\\n(c) A prompt template for few-shot learning (when a reviewer comment is available).\\n(Persona) You are an expert software developer in <lang>. You always want to improve your code to have higher quality. You have to generate an output that follows the given examples.(Instruction and examples) You are given 3 examples. Each example begins with \"##Example\" and ends with \"---\". Each example contains the submitted code and the improved code. The submitted code and improved code is written in <lang>. Your task is to improve your submitted code.## ExampleSubmitted code: <code>Improvedcode: <code>--<other examples>--(Input) Submitted code: <input code>\\n(d) A prompt template for few-shot learning (when a reviewer comment is not available).\\nFigure 2: Prompt templates for performing zero-shot learning and few-shot\\nlearning with GPT-3.5 ( lang refers to a programming language).\\nlowing the guidelines from OpenAI34to ensure that the struc-\\n3https: //help.openai.com /en/articles /6654000-best-practices-for-prompt-\\nengineering-with-openai-api\\n4https: //platform.openai.com /docs/guides /prompt-engineering /strategy-\\nwrite-clear-instructionsture of the prompt is suitable for GPT-3.5. In particular, the\\nprompt template for zero-shot learning (as depicted in Figure 2a\\nand Figure 2b) consists of the following components: perso-\\nna [26], instruction and input (i.e., submitted code). The input\\nalso includes a reviewer comment if available. On the other\\nhand, the prompt template for few-shot learning (as depicted in\\nFigure 2c and Figure 2d) consists of components similar to the\\nones in the prompt template for zero-shot learning, and a set of\\ninput (i.e., submitted code and a reviewer comment) and output\\n(i.e., improved code) example pairs. We include persona in both\\nprompt templates to instruct GPT-3.5 to act as a software devel-\\noper. We do so to ensure that the improved code generated by\\nGPT-3.5 looks like the source code written by a software devel-\\noper. For the prompt template for few-shot learning, we provide\\na set of example pairs to help GPT-3.5 adapt to the new task that\\nit has never learned before.\\n3.3. Zero-Shot Learning\\nTo perform zero-shot learning with GPT-3.5, we construct\\ninput prompts from the prompt template in Figure 2a and the\\ncode submitted for review with a reviewer comment in the test-\\ning set. However, in case a reviewer comment is not available,\\nwe construct input prompts from the prompt template in Fig-\\nure 2b and the code submitted for review instead. Then, we\\nask GPT-3.5 to generate the improved code from a given input\\nprompt.\\n3.4. Few-Shot Learning\\nTo perform few-shot learning with GPT-3.5, we first obtain\\nthree example pairs of input and output from the training set\\nby using BM25 [25] provided by the gensim5package. We do\\nso since prior work [52, 53] shows that BM25 [25] can outper-\\nform other sample selection approaches. Then, we construct\\ninput prompts from the prompt template in Figure 2c, the code\\nsubmitted for review and reviewer comment in the testing set,\\nand its corresponding selected examples. In case a reviewer\\ncomment is not available, we construct input prompts from the\\nprompt template in Figure 2d, the code submitted for review in\\nthe testing set, and its corresponding selected examples instead.\\nAfter that, we ask GPT-3.5 to generate the improved code from\\na given input prompt.\\n5https: //github.com /piskvorky /gensim\\n4', metadata={'page': 3, 'source': '/home/aravind-pt7506/college/project/ollama-examples/gcse/scientific papers/papers/2402.00905.pdf'})]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Title: Improving Code Quality using GPT-3.5 for Zero-Shot and Few-Shot Learning\n",
      "\n",
      "Key Points:\n",
      "1. The study explores the application of GPT-3.5, a large language model from OpenAI, to improve software code quality through zero-shot and few-shot learning.\n",
      "2. The authors provide prompt templates for both zero-shot and few-shot learning, which consist of persona, instruction, input (submitted code and reviewer comment), and output (improved code).\n",
      "3. For zero-shot learning, the model is given a single input prompt consisting of the submitted code with a reviewer comment if available, and its task is to generate improved code based on that.\n",
      "4. For few-shot learning, the model is provided with three example pairs of input (submitted code with a reviewer comment) and output (improved code), and its task is to learn from these examples and generate improved code for new inputs based on that knowledge.\n",
      "5. The authors use BM25 from the gensim package for selecting relevant examples to help the model adapt to new tasks during few-shot learning.\n",
      "\n",
      "Technical Details:\n",
      "1. Zero-shot Learning Prompt Template: Persona (expert software developer), Instruction (\"Your task is to improve given code\"), Input (submitted code with a reviewer comment), Output (improved code).\n",
      "2. Few-Shot Learning Prompt Template: Persona (expert software developer), Instruction (\"Your task is to improve given code\"), Input (submitted code with a reviewer comment and three selected examples), Output (improved code).\n",
      "3. Example selection using BM25 from gensim package.\n",
      "\n",
      "Formula/Table/Image: N/A\n",
      "\n",
      "The authors propose using GPT-3.5, a large language model from OpenAI, to improve software code quality by applying zero-shot and few-shot learning techniques. The study provides prompt templates for both approaches that include persona, instruction, input (submitted code with reviewer comment if available), and output (improved code). For zero-shot learning, the model is given a single input prompt consisting of submitted code with a reviewer comment if available, while for few-shot learning, it's provided with three example pairs to help adapt to new tasks. The authors use BM25 from gensim package for selecting relevant examples during few-shot learning.\n"
     ]
    }
   ],
   "source": [
    "print(res['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
